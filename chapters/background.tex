%!TEX root = ../dissertation.tex
\begin{savequote}[75mm]
Nulla facilisi. In vel sem. Morbi id urna in diam dignissim feugiat. Proin molestie tortor eu velit. Aliquam erat volutpat. Nullam ultrices, diam tempus vulputate egestas, eros pede varius leo.
\qauthor{Quoteauthor Lastname}
\end{savequote}

\chapter{Background and Related Work}
\label{ch:background}

\section{Previous Work by Genter and Stone}
Our work strongly builds upon the work of Genter and Stone, who have recently
published a series of papers on the best way to influence an existing flock to
change direction \cite{genter2015placement, genter2014neighborsorientherd,
genter2013visionstationary, genter2013backsearch,
genter2016facegoalfacecurrent, genter201612steplookahead}.
This prior literature has studied a number of placement strategies and
influencing agent behaviors, including questions of how best to join or leave
a flock in real scenarios.
Genter's PhD thesis also presents results from simulations with a slightly
different implementation of Reynold's flocking model, as well as physical
experiments with these algorithms in a small RoboCup setting.

This work has studied influencing agents in a number of contexts, including
convergence in a small setting and steering a small, dense flock around an
obstacle.
We are primarily interested in building upon the work from the former context,
although the second context is still interesting.
In their experiments, Genter and Stone place $180$ flocking agents and $20$
influencing agents on a $150\times150$ toroidal grid, and let agents influence
each other if they are in a neighborhood radius of $20$ units or less from each
other.
As a result, they simulate the dynamics of an extremely high-density flock.
In our work, we introduce new settings to study dynamics in low-density flocks.
We will introduce the formal flocking model and discuss Genter and Stone's
placement strategies and algorithms in more detail in \S\ref{ch:problem} and
\S\ref{ch:influencing}, but we will summarize Genter and Stone's key results
here to help situate our results.

Genter and Stone introduce a number of behaviors for influencing agents and
compare them against two baselines: \textit{face} and \textit{offset momentum}.
In \textit{face}, influencing agents simply face a predetermined goal direction
at all times; in \textit{offset momentum}, influencing agents look at their
neighbors, calculate an average velocity vector, and choose a direction to
offset that velocity from the predetermined goal direction.
The most promising behavior they study is \textit{one step lookahead}; in this
behavior, influencing agents cycle through choices for every direction they
can take, and simulate one step of their neighbors for every choice they take.
The agents pick the choice that minimizes the average difference of their
neighbors from the goal direction.
Not surprisingly, \textit{one step lookahead} beats the baseline behaviors in
their experiments.
We will see that this is not the case in low-density settings and discuss the
reasons behind this surprising shift in \S\ref{ch:results}.

\section{Other Related Work in Flocking}
There is a rich history of work studying flocking, much of it concentrating on
flocking models originally proposed by Reynolds \cite{reynoldsmodel} and,
independently, Vicsek \cite{vicsek1995}.
The classical Reynolds model identifies three properties necessary for
flocking:
\begin{itemize}
    \item Alignment: a tendency for flocking agents to steer towards the
            average heading of their neighbors
    \item Avoidance: a tendency for flocking agents to avoid collisions with
            their neighbors
    \item Cohesion: a tendency for flocking agents to steer towards the average
            position of their neighbors
\end{itemize}
\noindent Separately, Vicsek proposed a flocking model mathematically
equivalent to the alignment portion of the Reynolds model.
As a result, many subsequent studies that build off the Reynolds or Vicsek
models, including the studies of Genter and Stone, only study the alignment
portion.

Jadbadbaie et. al. \cite{jad2003convergence} have studied this model from an
analytical perspective.
Two strong results from this work are that a group of Reynolds-Vicsek agents
in a toroidal setting will eventually converge regardless of initial
conditions, and that in the presence of a single agent with fixed orientation
(analogous to a single influencing agent), all the agents will converge to that
fixed agent's orientation.
This provides important context for Genter and Stone's work and the work that
we present here; when the setting is toroidal, convergence is guaranteed, so
the interesting question how fast we can reach convergence.

Couzin et. al. \cite{couzin2005} have studied the design of influencing agents
for flocking as well, albeit with a slightly different model.
They have proposed an influencing behavior wherein influencing agents adopt
orientations ``in between" their desired goal orientation and the orientations
of their neighbors, in order to still influence their neighbors while not
adopting orientations so extreme that they have no chance of being effective in
the long term.
In \S\label{ch:influencing}, we adapt this algorithm to the Reynolds-Vicsek
flocking model to see how well it performs in our settings.

Han et. al. have \cite{han2010teleporting} published a series of papers showing
how to align a group of agents in the same direction.
This literature has assumed a single influencing agent with infinite speed, and
has used this property to construct a behavior that has the influencing agent
fly around and correct the orientation of agents one at a time.
The result is that the Reynolds-Vicsek agents all eventually converge to the
target direction, but are not connected to each other.
In our work, we limit the speed of influencing agents to be the same as the
Reynolds-Vicsek agents to prevent the use of behaviors like this, and in hopes
that our results will be more applicable to real applications; we suspect that
influencing agents that act similarly to real birds will be more successful in
real-world applications.

Su. et. al. \cite{su2009virtualleaderinformed} have also studied the question
of flock formation and convergence, but have studied the question in the
context of the Olfati-Saber flocking model
\cite{olfati2006virtualleaderinformed}.
This model assumes the existence of a single virtual leader that
non-influencing agents know about.
The virtual leader plays the role of an influencer here, but has special
control over the other agents based on its status.
In our work, we assume that influencing agents do not have any special
interaction rules with Reynolds-Vicsek agents.

Other researchers have tackled this question with real flocking agents.
Halloy et. al. \cite{Halloy2007} have used robotic influencing agents to move
cockroaches to areas they would otherwise avoid.
Cockroaches display flocking behavior, but with very different models from the
one that we study.
In this case, Halloy et. al. have exploited the cockroaches' inability to
differentiate between real cockroaches and the robotic influencing agents.

Vaughan et. al. \cite{vaughan98} have used robotic influencing agents to herd
a flock of ducks (on the ground) to a goal position in a small caged area.
The approach here largely uses the robot agents to ``push" the ducks from a
distance, like a dog herding sheep.
The dynamics in this case are very different from the models we study; when the
ducks are on the ground, they can stand still, for instance, and the fence
limits the ducks' behavior.

\section{Genetic Programming}
Genetic programming \cite{kozaGP} uses evolutionary algorithms to explore the
solution space of possible programs to optimize some metric.
Common to all genetic programming techniques is some sort of genome that can be
evolved over; in our work, this is a hand-designed domain-specific language
that takes a list of neighbors as input and outputs an orientation for the
influencing agent to face.
We directly evolve the programs that define our influencing agents' behavior.
Other choices for genome include neural networks or other data structures that
can take the relevant input and generate a desired output; we do not explore
these in our work.

The evolutionary algorithms themselves can vary in specifics, but they all
require some mutation function and fitness function, and sometimes a crossover
function.
A mutation function takes a genome and ``mutates" it in some way to produce
another genome.
In our work, this can happen by changing the value of a constant or a function
somewhere in the abstract syntax tree; in a genome defined by a neural network,
this can take the shape of changing the weights between some number of nodes.
The fitness function is used to evaluate individual genomes; each generation,
the output from the fitness function is used to select some number of genomes
to mutate or carry on into the next generation (survival of the fittest).
Finally, some evolutionary algorithms, including ours, use a crossover function
that takes two genomes and creates a new one from aspects of both (to simulate
sexual reproduction).

We are not the first researchers to apply genetic algorithms to the domain
of controlling agents in swarms; Dorigo and Trianni used genetic algorithms to
evolve controllers for s-bots, swarming robots designed to self-assemble and
move together as a swarm-bot \cite{DorigoSwarmBot}.
Their problem formulation is slightly different than ours, however.
In their project, they have complete control over every agent in the swarm;
the goal of their genetic algorithm is to produce a program that can create
collective behavior when run on every agent.
In our work, we only have control over a limited number of influencing agents.
In their work, Dorigo and Trianni use a neural network from sensors to the
actuators of their robots as their genome.
Although it would be interesting to apply this approach to our problem, we do
not do so in this work.

Instead, we use genetic programming over a domain specific language designed
with domain expertise.
This is similar in approach to Sean Luke's use of genetic programming to build
a team for the RoboCup97 competition \cite{lukeRoboCup97}.
One major difference is that our language is simpler than his, since the
virtual soccer players are capable of much more complex behaviors than our
flocking agents.
As a result, his language contained many mroe primitives than the one we
present in our work.
The execution model used by the RoboCup players was designed to simplify the
work to be done by the evolved program without unnecessarily biasing it toward
any particular strategy.
We have similar goals when designing our own execution model and language.

Much work has been done on developing techniques for evolving programs in
richer languages with complex constraints on which programs are valid
\cite{BriggsGP}.
Although we use functional programming ideas in our execution model, we keep
our language simple, with only conditional expressions for control flow
structures and only a single type.
Exploring a richer language with a type system to reduce the number of useless
programs our genetic algorithm searches would be very interesting, but we leave
it as future work.

