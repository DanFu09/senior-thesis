%!TEX root = ../dissertation.tex
\begin{savequote}[75mm]
Nulla facilisi. In vel sem. Morbi id urna in diam dignissim feugiat. Proin molestie tortor eu velit. Aliquam erat volutpat. Nullam ultrices, diam tempus vulputate egestas, eros pede varius leo.
\qauthor{Quoteauthor Lastname}
\end{savequote}

\chapter{Algorithms for Influencing Agents}
\label{ch:influencing}

\section{Placement Strategies (Barbara paper)}
In this work, we study a number of different placement strategies, shown in
Figure $\ref{fig:placements}$.
Except for slight modifications to make some of these strategies circular, all
the strategies are drawn from the literature \cite{genter2015placement,
genterthesis}.
\begin{figure}
    \centering
    \includegraphics[width=0.5\textwidth]{placements}
    \caption{The different placement strategies we explore in this paper.
    Red agents are influencing agents, and white agents are Reynolds-Viscek
    agents.
    Note that k-means is the only placement strategy where the placement of
    influencing agents depends on placement of Reynolds-Viscek agents.}
    \label{fig:placements}
\end{figure}
We note that the question of how to maneuver influencing agents to reach the
positions given by these placement strategies is important, but out of scope
for this paper.
For a discussion of this question, we refer the reader to Genter and Stone
\cite{genter2016facegoalfacecurrent, genterthesis}.

For the \textit{large} setting, we study three placement strategies:
\textit{random}, \textit{grid}, and \textit{k-means}.
The random placement strategy, as its name suggests, places influencing agents
randomly throughout the grid.
The grid placement strategy computes a square lattice on the grid and places
influencing agents on the lattice points.
This strategy ensures regular placement of influencing agents throughout the
grid.
The k-means placement strategy uses a k-means clustering algorithm on the
positions of Reynolds-Viscek agents in the simulation space.
This strategy finds a cluster for each influencing agent by setting $k$ equal
to the number of influencing agents, and then places an influencing agent at
the center of each cluster.

We develop similar placement strategies for our herd setting, with some
differences.
To adapt the strategies to a circular arrangement of agents, we define each
strategy in terms of some radius $r$ about an origin $O$, except for the
\textit{k-means} strategy, which remains the same.
We modify the \textit{random} placement strategy to randomly distribute agents
within the circle of radius $r$ about the origin $O$, instead of the entire
simulation space.
We adapt the grid placement strategy to a circular setting using a sunflower
spiral \cite{segermansunflower}.
In polar coordinates relative to $O$, the position of the $n$-th influencing
agent in a sunflower spiral is given by $(c\sqrt{n}, \frac{2\pi}{\phi^2}n)$,
where $\phi$ is the golden ratio, and $c$ is a normalizing constant such that
the last influencing agent has distance $r$ from $O$.
We also introduce a circle border placement strategy, inspired from the border
strategies from \cite{genter2015placement}.
This strategy places agents on the circumference of the circle of radius $r$
around the origin $O$.
We refer to the circular strategies as \textit{circle-random},
\textit{circle-grid}, and \textit{circle-border}, respectively.

\section{Behaviors (Barbara paper)}
NOTE: POSSIBLY GET RID OF GLOBAL/LOCAL DISTINCTION?

MENTION THAT WE INTRODUCE HAND-CONSTRUCTED BEHAVIORS HERE

ALSO ADD NEW BEHAVIORS THAT WE STUDY FOR THESIS

NOTE: UPDATE WHEN CAMERA-READY VERSION OF BARBARA PAPER IS DONE

Once we have placed our influencing agents, we still need to design how they
will work together to influence the flock.
We call this aspect of the design influencing agent behaviors.
In the present work we focus on decentralized ``ad-hoc" algorithms for our
influencing agents since this class of algorithms has been the focus of the
existing multiagent systems literature on this topic
\cite{genterthesis, genter2014neighborsorientherd, genter201612steplookahead}.

To help organize our analysis, we split behaviors into local and global
components; each influencing agent behavior is composed of a local and global
component.
The local component takes a desired orientation as an argument and dictates
which direction an influencing agent will face to try to influence its neighbors
towards the goal orientation.
The global component, on the other hand, dictates the desired orientation to
feed to the local component and how the influencing agents might coordinate to
decide on a desired orientation.
For example, one behavior might be for the influencing agents to circle around
the center of a grid and get the Reynolds-Viscek agents to circle with them.
The global component would compute the desired orientation necessary at each
step to maintain the circling behavior, and the local component would compute
which exact direction to face to get the Reynolds-Viscek agents to face the
desired orientation at that step.
For grammatical sanity, we will refer to different candidates for local and
global components as ``local behaviors" and ``global behaviors" throughout the
rest of this paper; however, it is important to recognize that any actual
behavior must have a local and global component.

\subsubsection*{Local Behaviors}
Like our placement strategies, we draw on Genter and Stone for many of our
local behaviors \cite{genter201612steplookahead,genter2016facegoalfacecurrent,
genter2015placement}.
In previous work, they have introduced baseline behaviors \textit{face} and
\textit{offset momentum}, as well as more sophisticated behaviors \textit{one
step lookahead} and \textit{coordinated}.
Each of these behaviors requires a goal angle $\theta^*$.
In \textit{face}, influencing agents always face the angle $\theta^*$.
In \textit{offset momentum}, influencing agents calculate the average velocity
vector of the agents in their neighborhood, and take on a velocity vector that,
when added to the average velocity vector, sums to the vector pointing in
direction $\theta^*$.
In \textit{one step lookahead}, each influencing agent cycles through different
angles and simulates one step of each of its neighbors if it were to move in
that angle.
It adopts the angle that results in the smallest average difference in angle
from $\theta^*$ among all its neighbors.
Finally, in \textit{coordinated}, each agent pairs with another and runs a one
step lookahead to minimize the average difference in angle from $\theta^*$
among both their neighbors.
For a more detailed explanation of these behaviors, especially the
\textit{coordinated} behavior, we direct the reader to Genter and Stone
\cite{genter201612steplookahead}.

\begin{table}[]
\footnotesize
\centering
\caption{Summary of global behaviors we investigate}
\label{table:global}
\begin{tabular}{|l|l|l|l|}
\hline
\textbf{Setting}       & \textbf{Type}               & \textbf{Name}        & \textbf{Description}                    \\ \hline
\multirow{3}{*}{Large} & \multirow{3}{*}{\textbf{}}  & \textit{Direct}      & Influence neighbors or face goal        \\ \cline{3-4}
                       &                             & \textit{Random}      & Influence neighbors or face random      \\ \cline{3-4}
                       &                             & \textit{Multistep}   & \textit{\textbf{Follow-then-influence}} \\ \hline
\multirow{4}{*}{Herd}  & Net                         & \textit{Direct}      & Influence neighbors or face goal        \\ \cline{2-4}
                       & \multirow{3}{*}{Stationary} & \textit{Circle}      & Trace circle around agents              \\ \cline{3-4}
                       &                             & \textit{Polygon}     & Trace polygon around agents             \\ \cline{3-4}
                       &                             & \textit{Multicircle} & \textit{\textbf{Follow-then-influence}} \\ \hline
\end{tabular}
\end{table}


\subsubsection*{Global Behaviors}
The global behaviors we investigate are listed in Table \ref{table:global}.
We have three global behaviors for the \textit{large} setting: \textit{direct},
\textit{random}, and \textit{multistep}.
As its name suggests, the \textit{direct} global behavior has each influencing
agent use a local behavior to directly influence its neighbors towards the
goal angle $\theta^*$.
When an influencing agent has no neighbors, it simply faces the goal direction.

The \textit{random} global behavior is very similar, but it directs influencing
agents in a random direction when they have no Reynolds-Viscek neighbors.
We introduced this behavior as a response to interactions where an influencing
agent is almost successful in changing the direction of a group of Reynolds-Viscek
agents, but gets separated before it is completely successful.
In these cases, the Reynolds-Viscek agents are left on a trajectory that is almost
parallel to the goal direction; as a result, further interactions with
influencing agents are rare.

The \textit{multistep} behavior is a novel contribution and adopts what we call
a ``follow-then-influence" behavior.
In the initial stage, influencing agents simply behave like normal Reynolds-Viscek
agents; as a result, they easily join flocks and become distributed throughout
the grid.
At the same time, the influencing agents perform a global calculation of the
total number of Reynolds-Viscek agents that are path-connected to influencing
agents.
Here, we define two agents as being path-connected if there is a path between
them, where edges are created by two agents being in each other's neighborhood.
Once that number passes some threshold $T$, the influencing agents calculate
the average angle $\overline{\theta}$ among all the agents that are locally
connected to influencing agents, and from there adopt the \textit{direct}
behavior with goal $\overline{\theta}$.
We choose $\overline{\theta}$ to minimize the amount that each influencing
agent needs to turn its flock on average.
MENTION GLOBAL VIEW HERE AND THE SMALLER VERSION

For the \textit{herd} setting, we divide our global behaviors into two
categories: a \textit{net} behavior, and a set of \textit{stationary} behaviors.
As a reminder, in the \textit{herd} setting, the simulation space is
non-toroidal, and all the Reynolds-Viscek agents start in a circle in the
center.
In this setting, flock formation is not guaranteed, so we are interested in
using influencing agents to instigate flocking behavior.
There are two different choices we can make; we can either try to force the
Reynolds-Viscek agents to stay in the center (\textit{stationary} behaviors),
or we can let the influencing agents direct the Reynolds-Viscek agents away
from their initial starting position.
Since all our agents have a constant speed, the former is much more difficult
than the latter, so we must evaluate them separately.

The \textit{net} behavior is equivalent to the \textit{direct} behavior; there
is a single pre-determined goal direction, and the influencing agents try to
direct the Reynolds-Viscek agents towards the goal direction.
We call it a net behavior because it looks as if the influencing agents are
``catching" the Reynolds-Viscek agents in a net.

We study three \textit{stationary} behaviors: \textit{circle}, \textit{polygon},
and \textit{multicircle}.
The \textit{circle} and \textit{polygon} behaviors have each influencing agent
trace a circle or polygon around the origin.
For placement strategies where influencing agents have different distances to
the origin, the influencing agents simply trace circles and polygons of
different radii.

The \textit{multicircle} behavior is analogous to the \textit{multistep} behavior
from \textit{large}.
The influencing agents start out by circling around the origin and wait for
Reynolds-Viscek agents to enter their neighborhood.
Once they detect Reynolds-Viscek agents in their neighborhood, they adopt a
``following" behavior where they act like Reynolds-Viscek agents to integrate
into a small flock.
They continue this following stage until reaching a final radius $r_F$, at which
point they again adopt a circling behavior.
In addition to building influence by following before influencing, this behavior
also makes maintaining influence easier; since the final radius is larger than
the original radius, the final path turns less sharply than if the influencing
agents had stayed at their original radius.
To the best of our knowledge, this is the first presentation of such a
multi-stage behavior to induce circling behavior under the Reynolds-Vicsek model in
the literature.

Besides the above global behaviors, we also explored a few more variations on
the \textit{net} behavior and the \textit{multicircle} behavior.
One variation of the \textit{net} behavior was equivalent to the \textit{random}
global behavior from the large setting.
The other was similar to the stationary \textit{circle} behavior; influencing
agents would trace a circle until encountering Reynolds-Viscek agents,
whereupon they would influence the Reynolds-Viscek agents towards the goal
direction.
The variations on the \textit{multicircle} behavior included different final
radii from those presented in $\S\ref{sec:evaluation}$, as well as a variation
that introduced a transitional period between the initial following state and
the final circling state.
These additional variants added few qualitative insights, so we do not report
them for simplicity of exposition.

% This behavior is parameterized by three variables, a placement radius $r_P$, a
% threshold radius $r_T$ and a final radius $r_F$.
% A diagram of this behavior is shown in $\ref{fig:multicircle}$.
% \begin{figure}
%     \centering
%     \includegraphics[width=0.5\textwidth]{multicircle}
%     \caption{A diagram of the \textit{multicircle} global behavior. NOTE: WE WILL REPLACE THIS WITH A NON-SKETCH}
%     \label{fig:multicircle}
% \end{figure}
% Agents start by tracing a circle around the origin of radius $r_P$; $r_P$ is
% defined by the agent's placement under the placement strategy.
% Once the agents come into contact with Reynolds-Viscek agents, they follow the Reynolds-Viscek
% agents until they reach a threshold radius $r_T$.
% From there, they trace a transition path until they are a radius $r_F$ from the
% origin, where they settle into a final trajectory tracing the circle of radius
% $r_F$ around the origin.

% We compute the transition path by finding a circle that is tangent to both the
% influencing agent's velocity vector when it is at radius $r_T$ and the final
% circle of radius $r_F$.
% This problem reduces to one of geometry; we wish to find a point $O'$ around
% which to trace a transition circle.
% For the rest of this computation, assume that the influencing agent is at the
% threshold radius $r_T$.

% We can reduce the set of candidate points by taking the line orthogonal to the
% agent's velocity vector that intersects the agent's velocity vector at the
% agent's position.
% Consider a point $O'$ on this line, and let $r'$ be the distance of
% $O'$ from the agent.
% Then, the circle of radius $r'$ centered around $O'$ is tangent to
% the agent's velocity vector at the agent's position.
% Since we know the form of the line that $O'$ is on, we simply need to
% determine a proper radius $r'$.

% We have one more condition: that the transition circle be tangent to the larger
% circle.
% For this to be the case, we must have that the distance from $O'$ to the
% larger circle be $r'$.
% Let $r_{O}$ be the distance from $O'$ to the origin of the larger circle.
% Then we have $r' = r_F - r_{O}$.
% This gives us the proper value of $r'$, and two candidates for $O'$.
% We choose the point that causes the agent to enter the larger circle clockwise,
% to prevent collisions on the larger circle.
% During the actual simulations, we calculate this point numerically, walking
% along both sides of the orthogonal line until finding the right candidate.
